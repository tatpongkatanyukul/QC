# Nielsen and Chuang chapter 2


# Chapter 2

Page 69
> A diagonal representation for an operator $A$ on a vector $V$ is a representation $A = \sum_i \lambda_i \ket{i} \bra{i}$, where the vectors $\ket{i}$ form an orthonormal set of eigenvectors for $A$, with corresponding eigenvalues $\lambda_i$.
> An operator is said to be _diagonalizable_ if it has a diagonal representation.

My Q: What are the necessary and sufficient conditions for a matrix in Hilbert space to be _diagonalizable_?
A: 
> Theorem 168. Let $A$ be $n \times n$. If $A$ has as many as $n$ linearly independent eigenvectors, which means
```math
\begin{align}
rank(V) &= dim( \sum_{j=1}^k \mathbb{M}_j ) = n \\
span(V) &= \sum_{j=1}^k \mathbb{M}_j = \mathbb{R}^n
\end{align}
```
>then and only then $A$ is diagonalizable.
>
> Definition 157. Let an $n \times n$ matrix $A$ and let $\lbrace \lambda_1, \ldots, \lambda_n\rbrace$ its eigenvalues. 
> For each $\lambda_j$ we define the corresponding *characteristic manifold* or *eigenspace* $\mathbb{M}_j$ as the subspace of all eigenvectors associated with $\lambda_j$:
$$\mathbb{M}_j \equiv \lbrace x \in \mathbb{R}^n | A x = \lambda_j x \rbrace$$.
* See [Lecture Notes 14.102, Math for Economists](https://web.mit.edu/14.102/www/notes/lecturenotes0927.pdf)


$$\{ a_{ij} \}$$

## 2.1.6 Adjoints and Hermitian operators

Self-adjoint is Hermitian, i.e., if $A = \ket{v} \bra{v}$ then $A = A^\dagger$.

* It is so: consider $\ket{v} = [v_1, v_2, \ldots, v_k]^T$, so $\bra{v} = [v_1^\ast, v_2^\ast, \ldots, v_k^\ast]$.
  * Therefore, 
  $A = \ket{v} \bra{v}$ =
```math  
\begin{equation}
\begin{pmatrix}
  v_1 v_1^\ast & v_1 v_2^\ast & \cdots & v_1 v_k^\ast \\
  v_2 v_1^\ast & v_2 v_2^\ast & \cdots & v_2 v_k^\ast \\
  \ldots       & \ldots       & \ddots & \ldots \\
  v_k v_1^\ast & v_k v_2^\ast & \cdots & v_k v_k^\ast \\
\end{pmatrix}
\end{equation}
```
which is equal to $(A^T)^\ast$. That is, if $A = \ket{v} \bra{v}$ then $A = A^\dagger$. Q.E.D.


### Exercise 2.16
> Show that any projector $P$ satisfies the equation $P^2 = P$.

Given $P = \sum_i \ket{i} \bra{i}$ (from eq. 2.35) when $\ket{i}$'s are orthonormal bases, thus
$$P^2 = P \cdot P = (\sum_i \ket{i} \bra{i}) \cdot (\sum_j \ket{j} \bra{j})$$.
```math  
\begin{align}
P^2 &= \sum_i \sum_j \ket{i} \braket{i}{j} \bra{j} \\
 &= \sum_i \sum_{j \neq i} \ket{i} \bra{i}\ket{j} \bra{j} + \sum_i \ket{i} \bra{i}\ket{i} \bra{i} \\
 &= \sum_i \sum_{j \neq i} \ket{i} 0 \bra{j} + \sum_i \ket{i} 1 \bra{i} \\
 &= \sum_i \ket{i} \bra{i} = P.\\
\end{align}
```
Q.E.D.

### Exercise 2.17
> Show that a normal matrix is Hermitian if and only if it has real eigenvalues.

Recall all related properties:
* $A$ is a normal matrix if $A A^\dagger = A^\dagger A$.
* $A$ is Hermitian if $A = A^\dagger$.
* Adjoint $A^\dagger = (A^\ast)^T$.
* Eigenvalues
  * Eigenvalues of $A$ are the solutions $\lambda_i$'s of $det | A - \lambda I | = 0$.
  * Any matrix $A = \sum_i \lambda_i \ket{i} \bra{i}$ where $\ket{i}$'s form an orthonormal set of eigenvectors for $A$ with corresponding eigenvalues $\lambda_i$'s.

To clarify, if $A$ is normal, $A$ can be either Hermitian or not.
* If $A$ is Hermitian (Hermitian is always normal: $A = A^\dagger \rightarrow A A^\dagger = A^\dagger A$.

Other handy properties
* $(\ket{w} \bra{v})^\dagger = \ket{w} \bra{v}$. (page 70, Ex. 2.13)
* $(\sum_i a_i A_i)^\dagger = \sum_i a_i^\ast A_i^\dagger$. (page 70, Ex. 2.14, Eq. 2.33)

Given Hermitian, that is
$$A = A^\dagger$

LHS:
$$A = \sum_i \lambda_i \ket{i} \bra{i}$$

RHS:
```math
\begin{align}
A^\dagger &= (\sum_i \lambda_i \ket{i} \bra{i})^\dagger \\
&= \sum_i \lambda_i^\ast (\ket{i} \bra{i})^\dagger \mbox{(from eq. 2.33)} \\
&= \sum_i \lambda_i^\ast \ket{i} \bra{i} \mbox{(from ex. 2.13)} \\
\end{align}
```

To be Hermitian, LHS = RHS, i.e.,

$$A = \sum_i \lambda_i \ket{i} \bra{i} = \sum_i \lambda_i^\ast \ket{i} \bra{i} = A^\dagger$$,
which implies
$$\lambda_i = \lambda_i^\ast$$ for all $i$'s.

Therefore, $\lambda_i$ must be real, so that it is equal to its own conjugate. Q.E.D.

## Paragraph 1, page 71
> ... if $\ket{v_i}$ and $\ket{w_i}$ are any two orthonormal bases, then it is easily checked that the operator $U$ defined by $U \equiv \sum_i \ket{w_i} \bra{v_i}$ is a unitary operator.

Recall
* $U$ is unitary if $U^\dagger U = I$.

Given $U \equiv \sum_i \ket{w_i} \bra{v_i}$,
```math
\begin{align}
U^\dagger U &= (\sum_i \ket{w_i} \bra{v_i})^\dagger (\sum_j \ket{w_j} \bra{v_j}) \\
&= (\sum_i \ket{v_i} \bra{w_i}) \cdot (\sum_j \ket{w_j} \bra{v_j}) \\
&= \sum_i \sum_j \ket{v_i} \bra{w_i}\ket{w_j} \bra{v_j} \\
&= \sum_i \sum_{j \neq i} \ket{v_i} \bra{w_i}\ket{w_j} \bra{v_j} + \sum_i \ket{v_i} \bra{w_i}\ket{w_i} \bra{v_i}\\
&= \sum_i \sum_{j \neq i} \ket{v_i} 0 \bra{v_j} + \sum_i \ket{v_i} 1 \bra{v_i} \;\;\mbox{(orthonormal)}\\
&= \sum_i \ket{v_i} \bra{v_i}\\
\end{align}
```

Since $\ket{v_i}$'s form an orthonormal basis set, $\sum_i \ket{v_i} \bra{v_i} = I$ (from Eq. 2.22). Q.E.D.

### Exercise 2.18
> Show that all eigenvalues of a unitary matrix have modulus 1, that is, can be written in the form $e^{i \theta}$ from some real $\theta$.

Related properties
* unitary property $U^\dagger U = I$.
* Any matrix $A = \sum_i \lambda_i \ket{i} \bra{i}$ where $\ket{i}$'s form an orthonormal set of eigenvectors for $A$ with corresponding eigenvalues $\lambda_i$'s.

Consider $U^\dagger U$.
```math
\begin{align}
U^\dagger U &= (\sum_i \lambda_i \ket{i} \bra{i})^\dagger (\sum_j \lambda_j \ket{j} \bra{j}) \\
&= \sum_i \lambda_i^\ast (\ket{i} \bra{i})^\dagger (\sum_j \lambda_j \ket{j} \bra{j}) \\
&= \sum_i \lambda_i^\ast (\ket{i} \bra{i}) (\sum_j \lambda_j \ket{j} \bra{j}) \\
&= \sum_i \sum_j \lambda_i^\ast  \lambda_j \ket{i} \bra{i} \ket{j} \bra{j} \\
&= \sum_i \sum_{j \neq i} \lambda_i^\ast  \lambda_j \ket{i} \bra{i} \ket{j} \bra{j} + \sum_i \lambda_i^\ast  \lambda_i \ket{i} \bra{i} \ket{i} \bra{i}\\
&= \sum_i \sum_{j \neq i} \lambda_i^\ast  \lambda_j \ket{i} 0 \bra{j} + \sum_i \lambda_i^\ast  \lambda_i \ket{i} 1 \bra{i} \;\;\mbox{(orthonormal)}\\
&= \sum_i \lambda_i^\ast  \lambda_i \ket{i} \bra{i}
\end{align}
```

Recall that given $\lambda = a + i b$, $ $\lambda_i^\ast  \lambda_i = a^2 + b^2$, i.e., modulus $|\lambda|$. 
Therefore, if $U$ is unitary, thus $U^\dagger U = \sum_i |\lambda_i| \ket{i} \bra{i} = I$ and since $I = \sum_i \ket{i} \bra{i}$.
That is, $|\lambda_i|$ must be 1. Q.E.D.

### Exercise 2.19
> (Pauli matrices: Hermitian and unitary) Show that the Pauli matrices are Hermitian and unitary.

There are 4 types of Pauli matrices.

```math
I = 
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
```
That is: 
* $I = I^\dagger$ $\Rightarrow$ Hermitian 
* $I^\dagger I = I$ $\Rightarrow$ Unitary 

```math
X = 
\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}
```
That is: 
* $X = X^\dagger$ $\Rightarrow$ Hermitian 
* $X^\dagger X =$ 
```math
\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}
\cdot
\begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}
=
\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
= I
```
$\Rightarrow$ Unitary 

Similarly,
```math
Y = 
\begin{pmatrix}
0 & -i \\
i & 0
\end{pmatrix}
```
and 
```math
Z = 
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
```
are Hermitian and unitary.

### Exercise 2.20
> (Basis changes) Suppose $A'$ and $A''$ are matrix representations of an operator $A$ on a vector space $V$ with respect to two different orthonormal bases, $\ket{v_i}$ and $\ket{w_i}$. Then the elements of $A'$ and $A''$ are $A_{ij}' = \bra{v_i} A \ket{v_j}$ and $A''_{ij} = \bra{w_i} A \ket{w_j}$. Characterize the relationship between $A'$ and $A''$.


